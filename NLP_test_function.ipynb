{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKGeU8V6QjE+6/v7wTQEQz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#importing libraries\n","import os\n","import joblib\n","import pandas as pd\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from sklearn.metrics import classification_report"],"metadata":{"id":"YvxJWI6m9pDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') #mounting to google drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlIDKcb76cvR","executionInfo":{"status":"ok","timestamp":1746204767947,"user_tz":-60,"elapsed":23889,"user":{"displayName":"Aqsa Shabbir","userId":"07107228905368806810"}},"outputId":"db7c2d53-654d-413c-8bf8-e144234b04c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["target_dir = '/content/drive/My Drive/Colab Notebooks/NLP_Final/models'\n","model_path = os.path.join(target_dir, 'logreg_model.joblib')\n","\n","# Loading the model\n","model = joblib.load(model_path)"],"metadata":{"id":"EjreA8Qf6p4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ModelTester:\n","    def __init__(self, model_dir='models'):\n","\n","        # Load Logistic Regression model and vectorizer\n","        self.logreg = joblib.load(os.path.join(model_dir, 'logreg_model.joblib'))\n","        self.vectorizer = joblib.load(os.path.join(model_dir, 'tfidf_vectorizer.joblib'))\n","\n","        # Load DistilBERT model and tokenizer\n","        self.bert_model = DistilBertForSequenceClassification.from_pretrained(\n","            os.path.join(model_dir, 'distilbert_model'))\n","        self.bert_tokenizer = DistilBertTokenizer.from_pretrained(\n","            os.path.join(model_dir, 'distilbert_tokenizer'))\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.bert_model.to(self.device)\n","\n","    def predict(self, text, model_type='both'):\n","\n","        results = {}\n","\n","        if model_type in ['both', 'logreg']:\n","            X = self.vectorizer.transform([text])\n","            results['logreg'] = self.logreg.predict(X)[0]\n","\n","        if model_type in ['both', 'bert']:\n","            inputs = self.bert_tokenizer(text, return_tensors=\"pt\",\n","                                         truncation=True, padding=True).to(self.device)\n","            with torch.no_grad():\n","                logits = self.bert_model(**inputs).logits\n","            results['bert'] = torch.argmax(logits, dim=1).item()\n","\n","        return results\n","\n","    def evaluate(self, test_file='/content/drive/My Drive/Colab Notebooks/NLP_Final/test.csv'):\n","\n","        test_df = pd.read_csv(test_file)\n","        texts = test_df['tweet_text'].tolist()\n","        y_true = test_df['sentiment'].tolist()\n","\n","        # Mapping for BERT's numeric predictions to string labels\n","        label_mapping = {0: 'negative', 1: 'positive', 2: 'neutral'}\n","\n","\n","        # Get predictions\n","        y_pred_logreg = [self.predict(text, 'logreg')['logreg'] for text in texts]\n","        y_pred_bert = [label_mapping[self.predict(text, 'bert')['bert']] for text in texts]\n","\n","        # Generate reports\n","        print(\"Logistic Regression Performance:\")\n","        print(classification_report(y_true, y_pred_logreg))\n","\n","        print(\"\\nDistilBERT Performance:\")\n","        print(classification_report(y_true, y_pred_bert))\n","\n","        return {\n","            'logreg_report': classification_report(y_true, y_pred_logreg, output_dict=True),\n","            'bert_report': classification_report(y_true, y_pred_bert, output_dict=True)\n","        }\n","\n","if __name__ == \"__main__\":\n","    tester = ModelTester(model_dir='/content/drive/My Drive/Colab Notebooks/NLP_Final/models')\n","\n","    # Test single prediction\n","    sample_text = \"The president's speech was inspiring and thoughtful\"\n","    print(f\"\\nSample Prediction for: '{sample_text}'\")\n","    print(tester.predict(sample_text))\n","\n","    # Full evaluation\n","    tester.evaluate()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTJKSp6H4lg2","executionInfo":{"status":"ok","timestamp":1746205516023,"user_tz":-60,"elapsed":5191,"user":{"displayName":"Aqsa Shabbir","userId":"07107228905368806810"}},"outputId":"736a030e-6472-4771-f599-33a3e6b01563"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample Prediction for: 'The president's speech was inspiring and thoughtful'\n","{'logreg': 'positive', 'bert': 1}\n","\n","Running full evaluation...\n","Logistic Regression Performance:\n","              precision    recall  f1-score   support\n","\n","    negative       0.60      1.00      0.75         3\n","     neutral       1.00      1.00      1.00        13\n","    positive       1.00      0.94      0.97        34\n","\n","    accuracy                           0.96        50\n","   macro avg       0.87      0.98      0.91        50\n","weighted avg       0.98      0.96      0.96        50\n","\n","\n","DistilBERT Performance:\n","              precision    recall  f1-score   support\n","\n","    negative       0.30      1.00      0.46         3\n","     neutral       0.00      0.00      0.00        13\n","    positive       0.00      0.00      0.00        34\n","\n","    accuracy                           0.06        50\n","   macro avg       0.10      0.33      0.15        50\n","weighted avg       0.02      0.06      0.03        50\n","\n"]}]}]}